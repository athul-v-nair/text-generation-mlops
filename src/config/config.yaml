data:
  dataset_name: wikitext
  dataset_config: wikitext-2-raw-v1
  seq_length: 128   

training:
  epochs: 20     
  batch_size: 8

  dim_model: 256
  num_layers: 4
  num_heads: 4
  dim_ff: 1024
  dropout: 0.2

  learning_rate: 3e-4
  weight_decay: 0.05

  patience: 10

device: "cpu"